{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#TO work with fb meme dataset, factory reset the runtime and then use previously created csv dataset\nimport os\nos.environ['KAGGLE_CONFIG_DIR'] = '/content/'\n!kaggle datasets download -d parthplc/facebook-hateful-meme-dataset\npassword = 'KexZs4tn8hujn1nK'\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63yPNNSfOFqP","outputId":"ff675794-145a-4059-de2b-6d66b5574cda","execution":{"iopub.status.busy":"2024-06-24T16:43:44.909792Z","iopub.execute_input":"2024-06-24T16:43:44.910625Z","iopub.status.idle":"2024-06-24T16:44:17.335049Z","shell.execute_reply.started":"2024-06-24T16:43:44.910590Z","shell.execute_reply":"2024-06-24T16:44:17.334113Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Dataset URL: https://www.kaggle.com/datasets/parthplc/facebook-hateful-meme-dataset\nLicense(s): unknown\nDownloading facebook-hateful-meme-dataset.zip to /kaggle/working\n100%|██████████████████████████████████████▉| 3.34G/3.35G [00:30<00:00, 136MB/s]\n100%|███████████████████████████████████████| 3.35G/3.35G [00:30<00:00, 118MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip -qq /kaggle/working/facebook-hateful-meme-dataset.zip","metadata":{"id":"aMkOf2jmRtGC","execution":{"iopub.status.busy":"2024-06-24T16:44:17.337264Z","iopub.execute_input":"2024-06-24T16:44:17.337726Z","iopub.status.idle":"2024-06-24T16:44:45.194267Z","shell.execute_reply.started":"2024-06-24T16:44:17.337683Z","shell.execute_reply":"2024-06-24T16:44:45.193156Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YnB-m88xQNEZ","outputId":"a9965b2b-2bab-4626-b005-322dd3db9bb7","execution":{"iopub.status.busy":"2024-06-24T16:44:45.195699Z","iopub.execute_input":"2024-06-24T16:44:45.196008Z","iopub.status.idle":"2024-06-24T16:45:06.475759Z","shell.execute_reply.started":"2024-06-24T16:44:45.195979Z","shell.execute_reply":"2024-06-24T16:45:06.474617Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-or1y4n13\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-or1y4n13\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.16.2)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m77.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=afda9ffb9c17b1f14ad44e55ab528edc63f9bf680187c0db33fba37ea4fe5c50\n  Stored in directory: /tmp/pip-ephem-wheel-cache-c2mffxu4/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm /kaggle/working/facebook-hateful-meme-dataset.zip","metadata":{"id":"wJTQULflRxd-","execution":{"iopub.status.busy":"2024-06-24T16:45:06.478480Z","iopub.execute_input":"2024-06-24T16:45:06.478795Z","iopub.status.idle":"2024-06-24T16:45:07.927120Z","shell.execute_reply.started":"2024-06-24T16:45:06.478766Z","shell.execute_reply":"2024-06-24T16:45:07.925923Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# https://drive.google.com/drive/folders/1RzxW8-kjGVR21A2RriEg9PlDfW9AYhDB?usp=sharing","metadata":{"id":"_6zlEJLPSG1Q","execution":{"iopub.status.busy":"2024-06-24T16:45:07.928527Z","iopub.execute_input":"2024-06-24T16:45:07.928842Z","iopub.status.idle":"2024-06-24T16:45:07.933047Z","shell.execute_reply.started":"2024-06-24T16:45:07.928811Z","shell.execute_reply":"2024-06-24T16:45:07.932106Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# !gdown 1RzxW8-kjGVR21A2RriEg9PlDfW9AYhDB","metadata":{"id":"znu4XE2rRr3B","execution":{"iopub.status.busy":"2024-06-24T16:45:07.934573Z","iopub.execute_input":"2024-06-24T16:45:07.934873Z","iopub.status.idle":"2024-06-24T16:45:07.942180Z","shell.execute_reply.started":"2024-06-24T16:45:07.934839Z","shell.execute_reply":"2024-06-24T16:45:07.941491Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# https://drive.google.com/drive/folders/1RzxW8-kjGVR21A2RriEg9PlDfW9AYhDB?usp=share_link","metadata":{"id":"T-5xOCLzS7pE","execution":{"iopub.status.busy":"2024-06-24T16:45:07.943192Z","iopub.execute_input":"2024-06-24T16:45:07.943532Z","iopub.status.idle":"2024-06-24T16:45:07.953546Z","shell.execute_reply.started":"2024-06-24T16:45:07.943505Z","shell.execute_reply":"2024-06-24T16:45:07.952656Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!pip install gdown==4.6.0","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:45:07.954623Z","iopub.execute_input":"2024-06-24T16:45:07.954878Z","iopub.status.idle":"2024-06-24T16:45:20.579902Z","shell.execute_reply.started":"2024-06-24T16:45:07.954855Z","shell.execute_reply":"2024-06-24T16:45:20.578771Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting gdown==4.6.0\n  Downloading gdown-4.6.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown==4.6.0) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown==4.6.0) (2.32.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown==4.6.0) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown==4.6.0) (4.66.4)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown==4.6.0) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown==4.6.0) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==4.6.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==4.6.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==4.6.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==4.6.0) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==4.6.0) (1.7.1)\nDownloading gdown-4.6.0-py3-none-any.whl (14 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install --upgrade --no-cache-dir gdown","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:45:20.584081Z","iopub.execute_input":"2024-06-24T16:45:20.584467Z","iopub.status.idle":"2024-06-24T16:45:20.588786Z","shell.execute_reply.started":"2024-06-24T16:45:20.584425Z","shell.execute_reply":"2024-06-24T16:45:20.587908Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import gdown","metadata":{"id":"mrvvvAouTSRZ","execution":{"iopub.status.busy":"2024-06-24T16:45:20.590028Z","iopub.execute_input":"2024-06-24T16:45:20.590299Z","iopub.status.idle":"2024-06-24T16:45:20.889580Z","shell.execute_reply.started":"2024-06-24T16:45:20.590276Z","shell.execute_reply":"2024-06-24T16:45:20.888800Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# gdown.download_folder(id='1RzxW8-kjGVR21A2RriEg9PlDfW9AYhDB',use_cookies=True, remaining_ok=True)\n# # https://drive.google.com/drive/folders/1RzxW8-kjGVR21A2RriEg9PlDfW9AYhDB?usp=sharing\n\n\n\n!gdown --folder '1RzxW8-kjGVR21A2RriEg9PlDfW9AYhDB'","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASDHVClvTSTP","outputId":"a94924c1-6d70-4ecc-d05c-1f46f1725cc0","execution":{"iopub.status.busy":"2024-06-24T16:45:20.890758Z","iopub.execute_input":"2024-06-24T16:45:20.891200Z","iopub.status.idle":"2024-06-24T16:46:14.974841Z","shell.execute_reply.started":"2024-06-24T16:45:20.891172Z","shell.execute_reply":"2024-06-24T16:46:14.973893Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Retrieving folder list\nRetrieving folder 1kUtluCkLZ6AAgbJSpD9fix6i3gGub-b5 ckpt_ijcai\nProcessing file 1LgUjTmgHt3ZUVtnPRxL3K6KNxYL8GUDn classifier (1).pt\nProcessing file 14bV8nOF_BdoZgOC_GazvZhXlycIxCk_O gpt2_backbone.pt\nRetrieving folder 1UgqTn0tfloqyzyjVRVkDzoWecY2lELuA full_ops\nProcessing file 16lKdhS2PtBXwwMwXwbMcKij7KO8j4cGL ijcai_500_ball_0.1_on_off.pkl\nProcessing file 1zreVymrlt1NjhzUndS0j2RkMRBBwwCe2 ijcai_500_ball_0.1_on_on.pkl\nProcessing file 1xe4eNAxxgVgGsIsAP_4P9x93oZAldcxU ijcai_500_ball_0.01_on_on.pkl\nProcessing file 1VLCzdflt_XoaZ0yNdiRxnN2jyDVUKgRW ijcai_500_ur_on_off.pkl\nProcessing file 1G6ZUsoqZrzHw3ow9gRfP86xMDGS_uYEm ijcai_500_ur_on_on.pkl\nProcessing file 1DDWUYMsbAbWr3Nn27CTivzBZ75WCsx_7 ijcai_1500_ball_0.1_on_off.pkl\nProcessing file 1WvYPzh3DzLbR570RVAy9ucQFCZfJPPlb ijcai_1500_ball_0.01_on_on.pkl\nProcessing file 1wGLFkDPe7VuTs2BUlVdZOZW79RnAj7EI ijcai_1500_ball_0.1_on_on.pkl\nProcessing file 1fxe3X_YtjTH99cc1sZePn4FnOccSkewe ijcai_1500_ur_on_off.pkl\nProcessing file 1oxbEFnj7uoXJdt-VwqXWl2CDI6zEeCsu ijcai_1500_ur_on_on.pkl\nProcessing file 11fNtwHrmQEof-9qZUgO8hax4mMQIbt44 ijcai_2500_ball_0.1_on_off.pkl\nProcessing file 1B_md7MTk0VDxBWmkseHGSvwySgVgzB1J ijcai_2500_ball_0.01_on_on.pkl\nProcessing file 1Ln5xrEAxMG0-mxB46UjPy0T0oCj6pNkl ijcai_2500_ball_0.1_on_on.pkl\nProcessing file 1s122tmAu0AjTnIRb3ybzPvd36iNP8jL5 ijcai_2500_ur_on_off.pkl\nProcessing file 1oCxUWriX65GGU0XJNAXPLsjdO65vc4l5 ijcai_2500_ur_on_on.pkl\nProcessing file 1vb9UtBO1EHGa4SK_SUcvIuW1AmhCKroG ijcai_3500_ball_0.1_on_off.pkl\nProcessing file 1E_jLXN6R9ivwfHjlonM5v195VKk9ud8e ijcai_3500_ball_0.1_on_on.pkl\nProcessing file 1J3s1Jlype3DQUhwAsWOeNT7IIc7URPmL ijcai_3500_ball_0.01_on_on.pkl\nProcessing file 1FiIgavqngDLaCWNLWnRxhtZTCF3wSsNC ijcai_3500_ur_on_off.pkl\nProcessing file 1-YhrO9vdD5NoMsgB9aTRBs0XpaWUK_w_ ijcai_3500_ur_on_on.pkl\nProcessing file 1JGv7CbdoEBzLfNVjQ8vselIzwAQYXGLG ijcai_baseline_ig.pkl\nProcessing file 1IU39uy6xisZZF71oLXdQ6Q_CArUR2u4T ijcai_baseline_ixg.pkl\nProcessing file 1oUUlljYbMxcXkYWnkvFf2zidnTLjINj3 ijcai_baseline_ks.pkl\nProcessing file 1WEJiEiRlAeFuJ3OI81c5NOh2-AR01Y1t ijcai_baseline_saliency.pkl\nProcessing file 1x0LUhtz8s4CwoAXGWCwfZJdP1UMnehid ijcai_ur_0.01_ball_on_on.pkl\nProcessing file 18V3HYYKWLJvu3FDHWhidi1-Yoztfvgju ijcai_ur_0.05_ball_on_on.pkl\nProcessing file 1-fRXw7HF-vePcjbGZRoLlO8ex9SQotVi ijcai_ur_ur_on_off.pkl\nProcessing file 1Moli75aiM2Eu2CU3CLWYQMt3cdRt-foQ ijcai_ur_ur_on_on.pkl\nProcessing file 1eIk9E2yiy9YJeN9uCaMy4uWKUs-Xrn_T kb_fb.pt\nRetrieving folder 17DU6yneRgdJW5dngIFbt2m9r3FzMXKZB tensors\nProcessing file 1Mabvh5vFL9mAyMA3AannjaiW9_Pj8qsA gl_.pt\nProcessing file 1s38GUgYNFcA3FblnrKaeh2NtLj2VG6rX gl.pt\nProcessing file 1qLToKaz9KGUD9v1774rCDnQw7R3y2Gze im_tensor_.pt\nProcessing file 1cpjd33deLMCFKNGvVIDweFiKF4GfC_Ad im_tensor.pt\nProcessing file 1mVYv3MYxAEVJpcqtMTa0GVDtnafFSmEJ img_id_.pt\nProcessing file 1m8IFIoY3ru6KhklEjGrZPfFzmJNEnxVl img_id.pt\nProcessing file 1D1Db1ZbomLw_Yh6NYF_LDWjJzr6cQPPH tx_tensor_.pt\nProcessing file 1V2pvRXed7-6QZBBbbZqCa31s1YNBr8KP tx_tensor.pt\nRetrieving folder list completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1LgUjTmgHt3ZUVtnPRxL3K6KNxYL8GUDn\nTo: /kaggle/working/ijcai_ckpts/ckpt_ijcai/classifier (1).pt\n100%|████████████████████████████████████████| 540M/540M [00:09<00:00, 55.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=14bV8nOF_BdoZgOC_GazvZhXlycIxCk_O\nTo: /kaggle/working/ijcai_ckpts/ckpt_ijcai/gpt2_backbone.pt\n100%|████████████████████████████████████████| 498M/498M [00:08<00:00, 57.5MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=16lKdhS2PtBXwwMwXwbMcKij7KO8j4cGL\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_500_ball_0.1_on_off.pkl\n100%|██████████████████████████████████████| 84.4k/84.4k [00:00<00:00, 89.3MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1zreVymrlt1NjhzUndS0j2RkMRBBwwCe2\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_500_ball_0.1_on_on.pkl\n100%|██████████████████████████████████████| 82.4k/82.4k [00:00<00:00, 90.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1xe4eNAxxgVgGsIsAP_4P9x93oZAldcxU\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_500_ball_0.01_on_on.pkl\n100%|██████████████████████████████████████| 80.6k/80.6k [00:00<00:00, 86.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1VLCzdflt_XoaZ0yNdiRxnN2jyDVUKgRW\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_500_ur_on_off.pkl\n100%|██████████████████████████████████████| 84.2k/84.2k [00:00<00:00, 85.3MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1G6ZUsoqZrzHw3ow9gRfP86xMDGS_uYEm\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_500_ur_on_on.pkl\n100%|██████████████████████████████████████| 82.2k/82.2k [00:00<00:00, 77.3MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1DDWUYMsbAbWr3Nn27CTivzBZ75WCsx_7\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_1500_ball_0.1_on_off.pkl\n100%|██████████████████████████████████████| 87.8k/87.8k [00:00<00:00, 71.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1WvYPzh3DzLbR570RVAy9ucQFCZfJPPlb\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_1500_ball_0.01_on_on.pkl\n100%|██████████████████████████████████████| 84.3k/84.3k [00:00<00:00, 70.6MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1wGLFkDPe7VuTs2BUlVdZOZW79RnAj7EI\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_1500_ball_0.1_on_on.pkl\n100%|██████████████████████████████████████| 85.8k/85.8k [00:00<00:00, 98.4MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1fxe3X_YtjTH99cc1sZePn4FnOccSkewe\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_1500_ur_on_off.pkl\n100%|██████████████████████████████████████| 87.8k/87.8k [00:00<00:00, 86.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1oxbEFnj7uoXJdt-VwqXWl2CDI6zEeCsu\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_1500_ur_on_on.pkl\n100%|██████████████████████████████████████| 85.8k/85.8k [00:00<00:00, 73.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=11fNtwHrmQEof-9qZUgO8hax4mMQIbt44\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_2500_ball_0.1_on_off.pkl\n100%|██████████████████████████████████████| 88.1k/88.1k [00:00<00:00, 86.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1B_md7MTk0VDxBWmkseHGSvwySgVgzB1J\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_2500_ball_0.01_on_on.pkl\n100%|██████████████████████████████████████| 85.4k/85.4k [00:00<00:00, 87.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Ln5xrEAxMG0-mxB46UjPy0T0oCj6pNkl\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_2500_ball_0.1_on_on.pkl\n100%|██████████████████████████████████████| 86.1k/86.1k [00:00<00:00, 84.4MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1s122tmAu0AjTnIRb3ybzPvd36iNP8jL5\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_2500_ur_on_off.pkl\n100%|██████████████████████████████████████| 88.0k/88.0k [00:00<00:00, 83.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1oCxUWriX65GGU0XJNAXPLsjdO65vc4l5\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_2500_ur_on_on.pkl\n100%|██████████████████████████████████████| 86.0k/86.0k [00:00<00:00, 95.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1vb9UtBO1EHGa4SK_SUcvIuW1AmhCKroG\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_3500_ball_0.1_on_off.pkl\n100%|██████████████████████████████████████| 88.1k/88.1k [00:00<00:00, 90.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1E_jLXN6R9ivwfHjlonM5v195VKk9ud8e\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_3500_ball_0.1_on_on.pkl\n100%|██████████████████████████████████████| 86.2k/86.2k [00:00<00:00, 93.2MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1J3s1Jlype3DQUhwAsWOeNT7IIc7URPmL\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_3500_ball_0.01_on_on.pkl\n100%|██████████████████████████████████████| 85.7k/85.7k [00:00<00:00, 87.9MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1FiIgavqngDLaCWNLWnRxhtZTCF3wSsNC\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_3500_ur_on_off.pkl\n100%|██████████████████████████████████████| 88.1k/88.1k [00:00<00:00, 90.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-YhrO9vdD5NoMsgB9aTRBs0XpaWUK_w_\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_3500_ur_on_on.pkl\n100%|██████████████████████████████████████| 86.2k/86.2k [00:00<00:00, 89.2MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1JGv7CbdoEBzLfNVjQ8vselIzwAQYXGLG\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_baseline_ig.pkl\n100%|██████████████████████████████████████| 79.9k/79.9k [00:00<00:00, 87.4MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1IU39uy6xisZZF71oLXdQ6Q_CArUR2u4T\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_baseline_ixg.pkl\n100%|██████████████████████████████████████| 80.4k/80.4k [00:00<00:00, 90.6MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1oUUlljYbMxcXkYWnkvFf2zidnTLjINj3\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_baseline_ks.pkl\n100%|██████████████████████████████████████| 78.8k/78.8k [00:00<00:00, 70.9MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1WEJiEiRlAeFuJ3OI81c5NOh2-AR01Y1t\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_baseline_saliency.pkl\n100%|██████████████████████████████████████| 82.8k/82.8k [00:00<00:00, 62.6MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1x0LUhtz8s4CwoAXGWCwfZJdP1UMnehid\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_ur_0.01_ball_on_on.pkl\n100%|██████████████████████████████████████| 87.9k/87.9k [00:00<00:00, 87.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=18V3HYYKWLJvu3FDHWhidi1-Yoztfvgju\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_ur_0.05_ball_on_on.pkl\n100%|██████████████████████████████████████| 88.3k/88.3k [00:00<00:00, 78.4MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-fRXw7HF-vePcjbGZRoLlO8ex9SQotVi\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_ur_ur_on_off.pkl\n100%|██████████████████████████████████████| 88.7k/88.7k [00:00<00:00, 68.9MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Moli75aiM2Eu2CU3CLWYQMt3cdRt-foQ\nTo: /kaggle/working/ijcai_ckpts/full_ops/ijcai_ur_ur_on_on.pkl\n100%|██████████████████████████████████████| 86.2k/86.2k [00:00<00:00, 79.0MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1eIk9E2yiy9YJeN9uCaMy4uWKUs-Xrn_T\nTo: /kaggle/working/ijcai_ckpts/kb_fb.pt\n100%|███████████████████████████████████████| 3.52M/3.52M [00:00<00:00, 193MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Mabvh5vFL9mAyMA3AannjaiW9_Pj8qsA\nTo: /kaggle/working/ijcai_ckpts/tensors/gl_.pt\n100%|██████████████████████████████████████| 1.38k/1.38k [00:00<00:00, 7.25MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1s38GUgYNFcA3FblnrKaeh2NtLj2VG6rX\nTo: /kaggle/working/ijcai_ckpts/tensors/gl.pt\n100%|██████████████████████████████████████| 17.4k/17.4k [00:00<00:00, 51.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1qLToKaz9KGUD9v1774rCDnQw7R3y2Gze\nTo: /kaggle/working/ijcai_ckpts/tensors/im_tensor_.pt\n100%|█████████████████████████████████████████| 513k/513k [00:00<00:00, 109MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1cpjd33deLMCFKNGvVIDweFiKF4GfC_Ad\nTo: /kaggle/working/ijcai_ckpts/tensors/im_tensor.pt\n100%|██████████████████████████████████████| 8.70M/8.70M [00:00<00:00, 25.6MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1mVYv3MYxAEVJpcqtMTa0GVDtnafFSmEJ\nTo: /kaggle/working/ijcai_ckpts/tensors/img_id_.pt\n100%|██████████████████████████████████████| 11.1k/11.1k [00:00<00:00, 40.3MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1m8IFIoY3ru6KhklEjGrZPfFzmJNEnxVl\nTo: /kaggle/working/ijcai_ckpts/tensors/img_id.pt\n100%|█████████████████████████████████████████| 195k/195k [00:00<00:00, 111MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1D1Db1ZbomLw_Yh6NYF_LDWjJzr6cQPPH\nTo: /kaggle/working/ijcai_ckpts/tensors/tx_tensor_.pt\n100%|█████████████████████████████████████████| 513k/513k [00:00<00:00, 109MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1V2pvRXed7-6QZBBbbZqCa31s1YNBr8KP\nTo: /kaggle/working/ijcai_ckpts/tensors/tx_tensor.pt\n100%|██████████████████████████████████████| 8.70M/8.70M [00:00<00:00, 13.6MB/s]\nDownload completed\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\nimport random\nimport torch\nimport numpy as np\nimport os\nfrom tqdm import tqdm\ntorch.use_deterministic_algorithms(True)\ndef set_seed(seed):\n\n    random.seed(seed)     # python random generator\n    np.random.seed(seed)  # numpy random generator\n\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\nfrom PIL import Image\n\n\n\nimport torch\nimport clip\nfrom PIL import Image\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Simulated batch sizes and vector dimensions\nbatch_size = 32\nvector_dim = 1024\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass fusion(nn.Module):\n    def __init__(self,img_feat_size, txt_feat_size, is_first, K, O, DROPOUT_R):\n        super(fusion, self).__init__()\n        #self.__C = __C\n        self.K = K\n        self.O = O\n        self.DROPOUT_R = DROPOUT_R\n\n        self.is_first = is_first\n        self.proj_i = nn.Linear(img_feat_size, K * O)\n        self.proj_t = nn.Linear(txt_feat_size, K * O)\n\n        self.dropout = nn.Dropout(DROPOUT_R)\n        self.pool = nn.AvgPool1d(K, stride = K)\n\n    def forward(self, img_feat, txt_feat, exp_in=1):\n\n        batch_size = img_feat.shape[0]\n        img_feat = self.proj_i(img_feat)\n        txt_feat = self.proj_t(txt_feat)\n\n        exp_out = img_feat * txt_feat\n        exp_out = self.dropout(exp_out) if self.is_first else self.dropout(exp_out * exp_in)\n        z = self.pool(exp_out) * self.K\n        z = F.normalize(z.view(batch_size, -1))\n        z = z.view(batch_size, -1, self.O)\n        return z\n\nimport os","metadata":{"id":"-SgbHGW7PNze","execution":{"iopub.status.busy":"2024-06-24T16:46:14.976645Z","iopub.execute_input":"2024-06-24T16:46:14.976980Z","iopub.status.idle":"2024-06-24T16:46:25.751003Z","shell.execute_reply.started":"2024-06-24T16:46:14.976947Z","shell.execute_reply":"2024-06-24T16:46:25.749827Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 156MiB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install jsonlines","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z40kMW20UjjK","outputId":"51bbb208-c905-4ab2-f9e0-1c8dd99c54f3","execution":{"iopub.status.busy":"2024-06-24T16:46:25.752592Z","iopub.execute_input":"2024-06-24T16:46:25.753409Z","iopub.status.idle":"2024-06-24T16:46:38.422567Z","shell.execute_reply.started":"2024-06-24T16:46:25.753372Z","shell.execute_reply":"2024-06-24T16:46:38.421244Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Collecting jsonlines\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines) (23.2.0)\nDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nInstalling collected packages: jsonlines\nSuccessfully installed jsonlines-4.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import jsonlines\n\nid2text = {}\nwith jsonlines.open('/kaggle/working/data/train.jsonl') as f:\n    for line in tqdm(f):\n\n        id2text[str(line['img']).split('/')[1]] = line['text']\n\nimport os\nimport json\nimport pickle\nfrom tqdm import tqdm\nimport requests\nimport pandas as pd\n\n#nlp = spacy.load(\"en_core_web_sm\")\n\n\n\nprefix = '/kaggle/working/ijcai_ckpts/tensors/'\nim_tensor = torch.load(prefix+'im_tensor.pt')\nim_tensor_ = torch.load(prefix+'im_tensor_.pt')\ntx_tensor = torch.load(prefix+'tx_tensor.pt')\ntx_tensor_ = torch.load(prefix+'tx_tensor_.pt')\n\ngl = torch.load(prefix+'gl.pt')\ngl_ = torch.load(prefix+'gl_.pt')\n\nkb_fb = torch.load('/kaggle/working/ijcai_ckpts/kb_fb.pt')\nimg_id = torch.load(prefix+'img_id.pt')\nimg_id_ = torch.load(prefix+'img_id_.pt')\n\nimport torch\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eXb7Lt5tPN1r","outputId":"e92ac4ce-2324-4177-cf48-759ccffb60a6","execution":{"iopub.status.busy":"2024-06-24T16:46:38.424107Z","iopub.execute_input":"2024-06-24T16:46:38.424469Z","iopub.status.idle":"2024-06-24T16:46:38.879876Z","shell.execute_reply.started":"2024-06-24T16:46:38.424438Z","shell.execute_reply":"2024-06-24T16:46:38.879103Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"8500it [00:00, 226929.83it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"class TinyModel(torch.nn.Module):\n\n    def __init__(self, mdl, mdl_rand, rand=False):\n        super(TinyModel, self).__init__()\n        if rand:\n            self.linear1 = nn.Sequential(nn.Linear(1024, 256), nn.ReLU(), nn.Linear(256, 512))\n        else:\n            self.linear1 = mdl_rand\n        self.activation = torch.nn.ReLU()\n        self.linear2 = torch.nn.Linear(512, 128)\n        self.linear3 = torch.nn.Linear(128, 2)\n        self.rand = rand\n        self.softmax = torch.nn.Softmax()\n        self.proj = torch.nn.Linear(512,768)\n        # self.proj = torch.nn.Linear(512,5120)\n\n    def forward(self, x, y):\n        joint_tensor = torch.cat((x, y), dim=1)\n        if self.rand:\n            m_ = self.linear1(joint_tensor)\n        else:\n            x = x.unsqueeze(1)\n            y = y.unsqueeze(1)\n            m_ = self.linear1(x, y).squeeze(dim=1)\n        m = self.activation(m_)\n        m = self.linear3(self.linear2(m))\n        # m = self.softmax(m)\n        return m, self.proj(m_)\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, tensor1, tensor2, gold_label, ids):\n        self.tensor1 = tensor1\n        self.tensor2 = tensor2\n        self.gl = gold_label\n        self.ids = ids\n\n    def __len__(self):\n        return len(self.tensor1)\n\n    def __getitem__(self, idx):\n        return self.tensor1[idx], self.tensor2[idx], self.gl[idx], self.ids[idx]\n\n# def collate_fn(batch):\n#     tensor1_batch, tensor2_batch = zip(*batch)\n#     return torch.stack(tensor1_batch), torch.stack(tensor2_batch)\n\n# Create your tensors\n#N = 100  # Example number of samples\ntensor1 = im_tensor\ntensor2 = tx_tensor\n\n# Create a custom dataset\ncustom_dataset = CustomDataset(tensor1, tensor2, gl, img_id)\ntrain_size = int(0.8 * len(custom_dataset))\ntest_size = len(custom_dataset) - train_size\n","metadata":{"id":"9jPoA9gWPN37","execution":{"iopub.status.busy":"2024-06-24T16:46:38.881160Z","iopub.execute_input":"2024-06-24T16:46:38.882323Z","iopub.status.idle":"2024-06-24T16:46:38.895577Z","shell.execute_reply.started":"2024-06-24T16:46:38.882283Z","shell.execute_reply":"2024-06-24T16:46:38.894675Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test_size","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:46:38.896711Z","iopub.execute_input":"2024-06-24T16:46:38.896975Z","iopub.status.idle":"2024-06-24T16:46:38.912005Z","shell.execute_reply.started":"2024-06-24T16:46:38.896952Z","shell.execute_reply":"2024-06-24T16:46:38.911078Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"1700"},"metadata":{}}]},{"cell_type":"code","source":"train_size","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:46:38.914824Z","iopub.execute_input":"2024-06-24T16:46:38.915124Z","iopub.status.idle":"2024-06-24T16:46:38.922459Z","shell.execute_reply.started":"2024-06-24T16:46:38.915101Z","shell.execute_reply":"2024-06-24T16:46:38.921430Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"6796"},"metadata":{}}]},{"cell_type":"code","source":"tx_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:46:38.923498Z","iopub.execute_input":"2024-06-24T16:46:38.923840Z","iopub.status.idle":"2024-06-24T16:46:38.932701Z","shell.execute_reply.started":"2024-06-24T16:46:38.923807Z","shell.execute_reply":"2024-06-24T16:46:38.931658Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"torch.Size([8496, 512])"},"metadata":{}}]},{"cell_type":"code","source":"set_seed(42)\ntrain_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n\n# Create a DataLoader with your collate_fn\nbatch_size = 4\nimport torch\ntorch.manual_seed(42)\n\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(42)\n\nset_seed(42)\ndataloader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=2,\n    worker_init_fn=seed_worker,\n    generator=g,\n)\n\n\n\n# dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n\n# Iterate through the DataLoader\ncounter = 0\nfor batch_tensor1, batch_tensor2, b3, b4 in dataloader:\n    print(\"Tensor 1 batch shape:\", batch_tensor1.shape)\n    print(\"Tensor 2 batch shape:\", batch_tensor2.shape)\n    print(\"Tensor 3 batch shape:\", b3.shape)\n    print(\"Tensor 4 batch shape:\", b4)\n    print(\"-\" * 30)\n    counter+=1\n    if counter==10:\n        break\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ut-dSbDMPN6I","outputId":"a359cbdc-dc01-4621-8d59-4f1cbfbd8901","execution":{"iopub.status.busy":"2024-06-24T16:46:38.933767Z","iopub.execute_input":"2024-06-24T16:46:38.934059Z","iopub.status.idle":"2024-06-24T16:46:39.042644Z","shell.execute_reply.started":"2024-06-24T16:46:38.934036Z","shell.execute_reply":"2024-06-24T16:46:39.041536Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Tensor 1 batch shape: torch.Size([4, 512])\nTensor 2 batch shape: torch.Size([4, 512])\nTensor 3 batch shape: torch.Size([4])\nTensor 4 batch shape: ('img/24859.png', 'img/96235.png', 'img/72084.png', 'img/21653.png')\n------------------------------\nTensor 1 batch shape: torch.Size([4, 512])\nTensor 2 batch shape: torch.Size([4, 512])\nTensor 3 batch shape: torch.Size([4])\nTensor 4 batch shape: ('img/58096.png', 'img/78215.png', 'img/38427.png', 'img/75016.png')\n------------------------------\nTensor 1 batch shape: torch.Size([4, 512])\nTensor 2 batch shape: torch.Size([4, 512])\nTensor 3 batch shape: torch.Size([4])\nTensor 4 batch shape: ('img/10976.png', 'img/63057.png', 'img/31764.png', 'img/95763.png')\n------------------------------\nTensor 1 batch shape: torch.Size([4, 512])\nTensor 2 batch shape: torch.Size([4, 512])\nTensor 3 batch shape: torch.Size([4])\nTensor 4 batch shape: ('img/49316.png', 'img/62439.png', 'img/29354.png', 'img/19532.png')\n------------------------------\nTensor 1 batch shape: torch.Size([4, 512])\nTensor 2 batch shape: torch.Size([4, 512])\nTensor 3 batch shape: torch.Size([4])\nTensor 4 batch shape: ('img/81576.png', 'img/21486.png', 'img/23504.png', 'img/74956.png')\n------------------------------\nTensor 1 batch shape: torch.Size([4, 512])\nTensor 2 batch shape: torch.Size([4, 512])\nTensor 3 batch shape: torch.Size([4])\nTensor 4 batch shape: ('img/07839.png', 'img/30487.png', 'img/97643.png', 'img/57412.png')\n------------------------------\nTensor 1 batch shape: torch.Size([4, 512])\nTensor 2 batch shape: torch.Size([4, 512])\nTensor 3 batch shape: torch.Size([4])\nTensor 4 batch shape: ('img/58924.png', 'img/64720.png', 'img/09267.png', 'img/48309.png')\n------------------------------\nTensor 1 batch shape: torch.Size([4, 512])\nTensor 2 batch shape: torch.Size([4, 512])\nTensor 3 batch shape: torch.Size([4])\nTensor 4 batch shape: ('img/82104.png', 'img/43190.png', 'img/73192.png', 'img/85496.png')\n------------------------------\nTensor 1 batch shape: torch.Size([4, 512])\nTensor 2 batch shape: torch.Size([4, 512])\nTensor 3 batch shape: torch.Size([4])\nTensor 4 batch shape: ('img/09462.png', 'img/75462.png', 'img/19753.png', 'img/37928.png')\n------------------------------\nTensor 1 batch shape: torch.Size([4, 512])\nTensor 2 batch shape: torch.Size([4, 512])\nTensor 3 batch shape: torch.Size([4])\nTensor 4 batch shape: ('img/05781.png', 'img/48756.png', 'img/68715.png', 'img/62705.png')\n------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\ndef get_tokens(prpmt,begin=False):\n    set_seed(42)\n    if begin:\n        prepended_inp = [tokenizer1.encode(i) for i in prpmt]\n    else:\n        prepended_inp = [tokenizer1.encode(i) for i in prpmt]\n    max_len = max([len(i) for i in prepended_inp])\n    #print(max_len)\n    attn_mask = []\n    bs = len(prpmt)\n    for i in range(bs):\n        tmp_len = max_len - len(prepended_inp[i])\n        tmp_mask = torch.tensor([1]* len(prepended_inp[i]) + [0]* tmp_len)\n        attn_mask.append(tmp_mask)\n        extra_tokens = tokenizer1.encode(tokenizer1.eos_token)*tmp_len\n        prepended_inp[i] = prepended_inp[i]+extra_tokens\n\n    attn_mask = torch.stack(attn_mask)\n    #print(prepended_inp)\n    #print(attn_mask, attn_mask.shape)\n    fin = []\n    for i in prepended_inp:\n        inter = []\n        for j in i:\n            inter.append(E[j,:])\n        fin.append(torch.stack(inter))\n\n\n\n\n\n\n    return torch.stack(fin), torch.tensor(prepended_inp), attn_mask\n\nfrom sklearn.metrics import *\n","metadata":{"id":"5QVW_NMOPN97","execution":{"iopub.status.busy":"2024-06-24T16:46:39.044212Z","iopub.execute_input":"2024-06-24T16:46:39.044526Z","iopub.status.idle":"2024-06-24T16:46:40.639583Z","shell.execute_reply.started":"2024-06-24T16:46:39.044494Z","shell.execute_reply":"2024-06-24T16:46:40.638607Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"_9n42MTCWCaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_performance_test():\n    torch.use_deterministic_algorithms(mode=True)\n    set_seed(42)\n    tinymodel.eval()\n    model1.eval()\n    #set_seed(42)\n    all_labs = []\n    all_labs_clf = []\n    counter = 0\n    for ii,ti, gol, ids in tqdm(test_dataset, position=0, leave=True):\n\n        #     print(0/0)\n        ti = ti.unsqueeze(0)\n        ii = ii.unsqueeze(0)\n        gol = [gol]\n        ids = [ids]\n        # print(batch_model1, batch_model1)\n\n        ids = list(map(lambda x: x.split('/')[1], ids))\n        #         ids_ = []\n        #         for i in range(len(ids)):\n        #             kbs = ids[i].split('[KB]')\n        #             caps = ids[i].split('[CAPTION]')[-1]\n        #             try:\n        #                 kb = '[KB] '+kbs[1]+'[KB] '+kbs[2] + '[CAPTION] '+ caps\n        #             except IndexError as e:\n        #                 kb = '[CAPTION] '+ caps\n        #             ids_.append(kb)\n        #         ids = ids_\n\n        ids_ = []\n        for i in range(len(ids)):\n            kbs = ids[i].split('[KB]')\n            #print('kbs ', kbs)\n            caps = ids[i].split('[CAPTION]')[-1]\n            #print(caps)\n            kb = ''\n            if len(kbs)>3:\n                kb = '[KB] '+kbs[1]+'[KB] '+kbs[2] + '[CAPTION] '+ caps\n            elif len(kbs)==3:\n                #print('in len3')\n                kb = '[KB] '+kbs[1]\n                caps = kbs[2].split('[CAPTION]')\n                kb += '[KB] '+ caps[0]+ ' [CAPTION] '+caps[1]\n            elif len(kbs)==2:\n                #print('in len 1')\n                caps = kbs[1].split('[CAPTION]')\n                kb += '[KB] '+ caps[0]+ ' [CAPTION] '+caps[1]\n            elif len(kbs)==1:\n                kb = kbs[0].strip()\n                #print('int ', kb)\n\n\n\n            ids_.append(kb)\n\n        ids = ids_\n\n\n\n        #         print(ids)\n        texts = [id2text[i] for i in ids]\n        ids = [kb_fb[i] for i in ids]\n\n        #     prpmt = []\n        #     for i,j in zip(ids,texts):\n        #         prpmt.append(i+'. The meme text reads :'+j + '. Classifier thinks the meme is')\n\n\n\n        i = ii.float().to(device)\n        t = ti.float().to(device)\n        batch_size = i.shape[0]\n        with torch.no_grad():\n            logits, m = tinymodel(i,t)\n        # print(m)\n        m = m.unsqueeze(dim=1)\n        all_labs_clf.append(logits.argmax(dim=-1)[0].item())\n        #         prmpt0, lab0 = get_tokens([tokenizer1.bos_token]*batch_size, begin=True)\n\n        #         print(prmpt0.shape)\n\n        gumbel_logits = F.gumbel_softmax(logits, tau=1, hard=True)\n        # print(gumbel_logits)\n        agmx = gumbel_logits.argmax(dim=1)\n\n        # print(agmx, logits.argmax(dim=1))\n        #         prpmt = ['model thinks the meme is ']*batch_size\n\n        #portion1,lab1, a1 = get_tokens(prpmt)\n\n        portion2,lab3, _ = get_tokens(['output of the classifier multimodal embedding is ']*batch_size)\n        lab4 = [tokenizer1.encode('-')[:] for i in range(batch_size)]\n        lab4 = torch.tensor(lab4)\n\n        # portion3 = get_tokens(['the meme is actually'])\n\n\n\n\n\n\n        kk = []\n        labs_verbalized = []\n        # batch_prompt = ['the model thinks the meme is ']*32\n        lab2 = []\n        for i in gumbel_logits:\n            agmx = i.argmax()\n            # print(agmx)\n            # print(dix[agmx.item()])\n            tokenized_ = tokenizer1.encode(dix[agmx.item()])[:]\n            labs_verbalized.append(dix[agmx.item()])\n            # print(tokenized_)\n            lab2.append(tokenized_)\n            embedding = E[tokenized_[0]]\n            one_hot = i.view(-1, 1)\n            # print(embedding.shape, one_hot, one_hot.shape)\n\n            e = torch.sum(one_hot*embedding.to(device), dim=0)\n            kk.append(e)\n        lab2 = torch.tensor(lab2)\n        prpmt = []\n        for i,j,z in zip(ids,texts,labs_verbalized):\n            prpmt.append(i+'. The meme text reads :'+j + '. Classifier thinks the meme is {}'.format(z))\n            #all_labs_clf.append(z)\n        portion1,lab1, a1 = get_tokens(prpmt)\n        string = ['the meme is actually']\n        portion3,lab5,_ = get_tokens(string)\n        inp_embed = torch.stack(kk).unsqueeze(dim=1)\n        # print(inp_embed)\n        # print(inp_embed.shape)\n        # print(portion1.shape,inp_embed.shape,portion2.shape,m.shape,portion3.shape)\n\n        final_embeds = torch.cat((portion1,inp_embed,portion2,m,portion3),dim=1)\n        # print(final_embeds.shape)\n        # print(lab1.shape, lab2.shape, lab3.shape, lab4.shape, lab5.shape)\n        labs_shape = torch.cat((lab2,lab3,lab4,lab5),dim=1).shape\n        remaining_mask = torch.ones(labs_shape[0], labs_shape[1]).long()\n        #print(a1)\n        #print(remaining_mask)\n\n        full_mask = torch.cat((a1,remaining_mask),dim=1)\n        #print('fm ', full_mask.shape)\n        labs = torch.cat((lab1,lab2,lab3,lab4,lab5),dim=1).to('cuda')\n        #print(full_mask)\n        #print(0/0)\n        #         print(final_embeds.shape, labs.shape)\n        #         print(0/0)\n        with torch.no_grad():\n            output2 = model1(inputs_embeds = final_embeds.float(), labels = labs.long(), attention_mask=full_mask.to('cuda'))\n        # print(output2.loss)\n        # print(0/0)\n        loss_lm = output2.loss\n        #print(tokenizer1.batch_decode(output2.logits.argmax(dim=-1)))\n\n        #print(loss_lm)\n        llm_lab = tokenizer1.decode(output2.logits.argmax(dim=-1)[0][-1])\n        all_labs.append(llm_lab)\n        counter+=1\n        #if counter==10:\n        #    break\n\n    return all_labs, all_labs_clf\n","metadata":{"id":"XOGIp-cOWCeK","execution":{"iopub.status.busy":"2024-06-24T16:46:50.117682Z","iopub.execute_input":"2024-06-24T16:46:50.118274Z","iopub.status.idle":"2024-06-24T16:46:50.143974Z","shell.execute_reply.started":"2024-06-24T16:46:50.118245Z","shell.execute_reply":"2024-06-24T16:46:50.142989Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"gl_test  = []\nfor _,_,g,_ in test_dataset:\n    gl_test.append(g)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:46:52.440556Z","iopub.execute_input":"2024-06-24T16:46:52.441184Z","iopub.status.idle":"2024-06-24T16:46:52.457878Z","shell.execute_reply.started":"2024-06-24T16:46:52.441125Z","shell.execute_reply":"2024-06-24T16:46:52.456959Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import torch\n#os.environ['CUBLAS_WORKSPACE_CONFIG']=:4096:8\ntorch.use_deterministic_algorithms(True)\nset_seed(42)\nimport numpy as np\nmodel1_name = 'gpt2'\ntokenizer1 = GPT2Tokenizer.from_pretrained(model1_name)\nmodel1 = GPT2LMHeadModel.from_pretrained(model1_name,output_hidden_states=True).to(device)\nmdl = fusion(512,512,True,256,512,0.1).to(device)\nmdl_rand = fusion(512,512,True,256,512,0.1).to(device)\ntinymodel = TinyModel(mdl,mdl_rand,rand=False).to(device)\nE = model1.transformer.wte.weight.detach()\n# E = model1.model.model.embed_tokens.weight.detach()\noptimizer = optim.Adam(tinymodel.parameters(), lr=0.0005)\noptimizer1 = optim.Adam(model1.parameters(), lr=0.0005)\ndix = {0:'normal', 1:'offensive'}\n\ntinymodel.train()\nmodel1.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:46:56.433988Z","iopub.execute_input":"2024-06-24T16:46:56.434720Z","iopub.status.idle":"2024-06-24T16:47:03.896554Z","shell.execute_reply.started":"2024-06-24T16:46:56.434684Z","shell.execute_reply":"2024-06-24T16:47:03.895653Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7033345d35c74ad9b518cbc38d6510a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47e41f693d4a42d0a0d0460b69037d0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3a52e6b573441cfb4568ddefdfa0109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ee96d99415c4939b636255c6b48a1e3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdf36879043c4330823bb4d2a7b11eed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f714f9f20d70469da32b22a1ef19c46b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7bfa0f0b984177bfa8986b1e7518d1"}},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"\n# Training loop\nnum_epochs = 10\ndef transpose(x):\n    return x.transpose(-2, -1)\n\nfor epoch in tqdm(range(num_epochs)):\n    #     set_seed(42)\n    l=0\n    cnt = 0\n    l_lm = 0\n    l_clf = 0\n    tinymodel.train()\n    model1.train()\n    for ii,ti, gol, ids in tqdm(dataloader, position=0, leave=True):\n        # print(batch_model1, batch_model1)\n       \n        ids = list(map(lambda x: x.split('/')[1], ids))\n        #    print(ids)\n        texts = [id2text[i] for i in ids]\n        #print('texts ',texts)\n        ids = [kb_fb[i] for i in ids]\n        \n        #         ids_ = []\n        #         for i in range(len(ids)):\n        #             kbs = ids[i].split('[KB]')\n        #             caps = ids[i].split('[CAPTION]')[-1]\n        #             try:\n        #                 kb = '[KB] '+kbs[1]+'[KB] '+kbs[2] + '[CAPTION] '+ caps\n        #             except IndexError as e:\n        #                 kb = '[CAPTION] '+ caps\n        #             ids_.append(kb)\n        #         ids = ids_\n\n        ids_ = []\n        for i in range(len(ids)):\n            kbs = ids[i].split('[KB]')\n            #print('kbs ', kbs)\n            caps = ids[i].split('[CAPTION]')[-1]\n            #print(caps)\n            kb = ''\n            if len(kbs)>3:\n                kb = '[KB] '+kbs[1]+'[KB] '+kbs[2] + '[CAPTION] '+ caps\n            elif len(kbs)==3:\n                #print('in len3')\n                kb = '[KB] '+kbs[1]\n                caps = kbs[2].split('[CAPTION]')\n                kb += '[KB] '+ caps[0]+ ' [CAPTION] '+caps[1]\n            elif len(kbs)==2:\n                #print('in len 1')\n                caps = kbs[1].split('[CAPTION]')\n                kb += '[KB] '+ caps[0]+ ' [CAPTION] '+caps[1]\n            elif len(kbs)==1:\n                kb = kbs[0].strip()\n                #print('int ', kb)\n\n\n\n            ids_.append(kb)\n        \n        ids = ids_\n        \n        \n        \n       \n        \n        i = ii.float().to(device)\n        t = ti.float().to(device)\n        batch_size = i.shape[0]\n        optimizer.zero_grad()\n        optimizer1.zero_grad()\n        logits, m = tinymodel(i,t)\n        # print(m)\n        m = m.unsqueeze(dim=1)\n        #         prmpt0, lab0 = get_tokens([tokenizer1.bos_token]*batch_size, begin=True)\n       \n        #         print(prmpt0.shape)\n\n        gumbel_logits = F.gumbel_softmax(logits, tau=1, hard=True)\n        # print(gumbel_logits)\n        agmx = gumbel_logits.argmax(dim=1)\n\n        # print(agmx, logits.argmax(dim=1))\n        #         prpmt = ['model thinks the meme is ']*batch_size\n\n        \n      \n        portion2,lab3, _ = get_tokens(['output of the classifier multimodal embedding is ']*batch_size)\n        lab4 = [tokenizer1.encode('-')[:] for i in range(batch_size)]\n        lab4 = torch.tensor(lab4)\n\n        # portion3 = get_tokens(['the meme is actually'])\n\n\n\n\n\n\n        kk = []\n        labs_verbalized = []\n        # batch_prompt = ['the model thinks the meme is ']*32\n        lab2 = []\n        for i in gumbel_logits:\n            agmx = i.argmax()\n            # print(agmx)\n            # print(dix[agmx.item()])\n            tokenized_ = tokenizer1.encode(dix[agmx.item()])[:]\n            labs_verbalized.append(dix[agmx.item()])\n            # print(tokenized_)\n            lab2.append(tokenized_)\n            embedding = E[tokenized_[0]]\n            one_hot = i.view(-1, 1)\n            # print(embedding.shape, one_hot, one_hot.shape)\n\n            e = torch.sum(one_hot*embedding.to(device), dim=0)\n            kk.append(e)\n        lab2 = torch.tensor(lab2)\n        #string = ['the meme is actually {}'.format(i) for i in labs_verbalized] # why this fucking mistake??????\n        \n        prpmt = []\n        #print('ids ',ids)\n        #print('texts1',texts)\n        #texts = [id2text[i] for i in ids]\n        #ids = [kb_fb[i] for i in ids]\n        for i,j,z in zip(ids,texts,labs_verbalized):\n            #print(i,j)\n            prpmt.append(i+'. The meme text reads :'+j + '. Classifier thinks the meme is {}'.format(z))\n        portion1,lab1, a1 = get_tokens(prpmt)\n        \n        string = ['the meme is actually {}'.format(dix[int(i)]) for i in gol]\n        portion3,lab5,_ = get_tokens(string)\n        inp_embed = torch.stack(kk).unsqueeze(dim=1)\n        # print(inp_embed)\n        # print(inp_embed.shape)\n        # print(portion1.shape,inp_embed.shape,portion2.shape,m.shape,portion3.shape)\n       \n        final_embeds = torch.cat((portion1,inp_embed,portion2,m,portion3),dim=1)\n        # print(final_embeds.shape)\n        # print(lab1.shape, lab2.shape, lab3.shape, lab4.shape, lab5.shape)\n        labs_shape = torch.cat((lab2,lab3,lab4,lab5),dim=1).shape\n        remaining_mask = torch.ones(labs_shape[0], labs_shape[1]).long()\n        #print(a1)\n        #print(remaining_mask)\n        \n        full_mask = torch.cat((a1,remaining_mask),dim=1)\n        #print('fm ', full_mask.shape)\n        labs = torch.cat((lab1,lab2,lab3,lab4,lab5),dim=1)\n        #print(full_mask)\n        #print(0/0)\n        #         print(final_embeds.shape, labs.shape)\n        #         print(0/0)\n        output2 = model1(inputs_embeds = final_embeds.float(), labels = labs.long(), attention_mask=full_mask.to('cuda'))\n        # print(output2.loss)\n        # print(0/0)\n        loss_lm = output2.loss\n\n\n\n\n        loss_clf = torch.nn.functional.cross_entropy(logits, gol.to(device), reduction='mean')\n\n        #print(loss_lm, loss_clf)\n       \n        loss = loss_lm+loss_clf\n\n        loss.backward()\n        optimizer.step()\n        optimizer1.step()\n\n\n        l+=loss.detach().item()\n        l_clf+=loss_clf.detach().item()\n        l_lm+=loss_lm.detach().item()\n        cnt+=1\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {l/cnt}, Loss clf: {l_clf/cnt}, Loss llm: {l_lm/cnt}\")\n    tinymodel.eval()\n    pred,_ = tinymodel(im_tensor_.float().to(device),tx_tensor_.float().to(device))\n    pred = pred.argmax(dim=1).detach().cpu().numpy()\n    print(f1_score(gl_, pred, average='macro'), accuracy_score(gl_, pred))\n    al, alc = get_performance_test()\n    l2l = {'normal':0, 'offensive':1}\n    ff = 0\n    al_ = []\n    for x in al:\n        if x.strip() in l2l:\n            al_.append(l2l[x.strip()])\n        else:\n            al_.append(0)\n            ff+=1\n    print('total outside {}'.format(ff))\n    #al_ = list(map(lambda x: l2l[x.strip()], al))\n    #alc_ = list(map(lambda x: l2l[x.strip()], alc))\n    print(f1_score(gl_test, al_, average='macro'), f1_score(gl_test, alc, average='macro'))\n    print(accuracy_score(gl_test, al_), accuracy_score(gl_test, alc))\n    \n    \nprint(\"Training complete!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:47:20.433167Z","iopub.execute_input":"2024-06-24T16:47:20.433534Z","iopub.status.idle":"2024-06-24T17:31:31.868055Z","shell.execute_reply.started":"2024-06-24T16:47:20.433505Z","shell.execute_reply":"2024-06-24T17:31:31.866703Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"100%|██████████| 1699/1699 [03:41<00:00,  7.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Loss: 1.9253241002314085, Loss clf: 0.562192445523322, Loss llm: 1.363131652971518\n0.554862139482406 0.596\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1700/1700 [00:42<00:00, 39.73it/s]\n 10%|█         | 1/10 [04:24<39:43, 264.83s/it]","output_type":"stream"},{"name":"stdout","text":"total outside 0\n0.4111616049656396 0.6718501091678935\n0.6470588235294118 0.7276470588235294\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1699/1699 [03:39<00:00,  7.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Loss: 1.3853795554021866, Loss clf: 0.44432260241554616, Loss llm: 0.9410569523803762\n0.619234345482773 0.638\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1700/1700 [00:42<00:00, 39.98it/s]\n 20%|██        | 2/10 [08:46<35:05, 263.13s/it]","output_type":"stream"},{"name":"stdout","text":"total outside 0\n0.7115580023876058 0.7120925697912328\n0.7382352941176471 0.7388235294117647\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1700/1700 [00:42<00:00, 39.98it/s]\n 30%|███       | 3/10 [13:08<30:38, 262.65s/it]","output_type":"stream"},{"name":"stdout","text":"total outside 0\n0.6494254810122316 0.6562695741356406\n0.73 0.7235294117647059\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1699/1699 [03:39<00:00,  7.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Loss: 0.7083478652252178, Loss clf: 0.16648378319442675, Loss llm: 0.5418640820856583\n0.5821888163730561 0.61\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1700/1700 [00:42<00:00, 39.85it/s]\n 40%|████      | 4/10 [17:31<26:15, 262.55s/it]","output_type":"stream"},{"name":"stdout","text":"total outside 0\n0.6431418994759374 0.6875425084693065\n0.7288235294117648 0.7329411764705882\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 923/1699 [01:58<01:40,  7.75it/s]IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n100%|██████████| 1699/1699 [03:37<00:00,  7.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Loss: 0.38453707634841366, Loss clf: 0.06236344833058966, Loss llm: 0.3221736280022824\n0.6220679114891354 0.636\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1700/1700 [00:42<00:00, 40.22it/s]\n 70%|███████   | 7/10 [30:31<13:02, 260.73s/it]","output_type":"stream"},{"name":"stdout","text":"total outside 0\n0.6780876158006715 0.696060606060606\n0.7252941176470589 0.7223529411764706\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1699/1699 [03:43<00:00,  7.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Loss: 0.34082681323901004, Loss clf: 0.055987616929647196, Loss llm: 0.28483919637949767\n0.5966634991025235 0.618\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1700/1700 [00:47<00:00, 36.14it/s]\n 80%|████████  | 8/10 [35:01<08:47, 263.89s/it]","output_type":"stream"},{"name":"stdout","text":"total outside 1\n0.6689436908575317 0.697249469249263\n0.7288235294117648 0.731764705882353\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 199/1699 [00:27<03:21,  7.43it/s]IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n100%|██████████| 1699/1699 [03:47<00:00,  7.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10], Loss: 0.2860599945421567, Loss clf: 0.04490537941484899, Loss llm: 0.24115461501358115\n0.6060734807017254 0.618\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1700/1700 [00:46<00:00, 36.87it/s]\n100%|██████████| 10/10 [44:11<00:00, 265.14s/it]","output_type":"stream"},{"name":"stdout","text":"total outside 0\n0.6924862382742815 0.7076764349195432\n0.7311764705882353 0.731764705882353\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}